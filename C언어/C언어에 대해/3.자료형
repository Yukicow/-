



char는 1바이트, short는 2바이트, int는 4바이트라는 식의 설명은 표준에 의하면 명백히 틀린 것이다.

밑에 그 이유를 알아 보자




1. 자료형이란


자료형이란 데이터를 표현하는 형태로서 이 자료는 메모리에 얼마만큼의 크기로, 

어떠한 형태로 저장되어 있는지를 알려 주는 역할을 합니다.

 

2. 유부호(signd)와 무부호(unsigned)


부호가 있는(signed) 자료형과 부호가 없는(unsigned) 자료형이 있다. 

부호가 있다는 표시를 명확하게 해주고 싶다면 signed int라고 표기할 수는 있지만 

부호 표기를 생략하면 signed가 기본이기 때문에(char는 예외) 그냥 int라고 표기하는 것이 일반적이다.

signed와 unsigned의 차이는, 예를 들어 signed int가 -100~100까지의 정수 표현이 가능하다고 한다면 

unsigned int는 0~200까지의 정수 표현이 가능한 차이가 있다.



3. 정수형


C의 표준으로는 정수형을 바이트로 지정하지 않고 비트로 최소 범위만을 지정하며 

구현(implementation)에 따라 범위가 달라지게 됩니다. 

따라서 char는 1바이트, short는 2바이트, int는 4바이트라는 식의 설명은 표준에 의하면 명백히 틀린 것이다.



3-1. char


흔히들 문자형이라고 하는 char(character)는 엄밀히 따지자면 정수형입니다.\

그러나 주로 ASCII 값 표현으로 사용하기 때문에 개념상 문자형이라고도 하는 것이죠.

# 한 마디로 정수형을 아스키 코드로 바꾸어 출력하는 것 뿐.


C 표준에서 char의 크기는 아래와 같다.


1. 최소 8비트

2. short 이하
 

1바이트가 아니고 최소 8비트이며 short 보다는 작거나 같아야 합니다.

때문에 char를 16비트로 하는 컴파일러가 있다 하더라도 C 표준에 맞습니다.


또한 C에서 1바이트는 8비트 기준이 아니라 CHAR_BIT(비트 수)을 기준으로 합니다.

CHAR_BIT가 16비트라면 sizeof(char)의 값은 1이지만 16비트인 것이죠. 

# sizeof(char)의 값은 항상 1(컴파일러 기준으로 1이 몇 비트인 지 정해짐)


때문에 C에서 자료형의 정확한 크기를 표현하기 위해서는 바이트가 아니라 비트로 표현하는 것이다. 

비트의 크기는 환경에 따라 달라지지 않기 때문.


다른 정수 자료형과는 달리 char는 char만 명시할 시 signed인지 unsigned인지는 시스템에 따라 달라진다.

따라서 CHAR_MIN이나 CHAR_MAX 값을 확인하면 현재 환경에서 char가 

signed char인지 unsigned char인지 알 수 있습니다. #일반적으로는 signed char


만약 char를 ASCII 값으로만 사용할 때는 7비트만 사용하기 때문에 변수 선언 시 

굳이 signed와 unsigned를 명시할 필요는 없지만 CHAR_BIT에 맞는 정수형으로 사용할 때는 

반드시 signed나 unsigned를 명시하는 것이 안전합니다.



3-2. short


short 크기의 표준은 아래와 같습니다.


1. 최소 16비트

2. char 크기 이상, int 크기 이하


일반적으로 short는 2바이트(16비트), int는 4바이트(32비트)이기 때문에 

int보다 작은 값의 정수를 사용할 때 short를 사용한다. 

다만 대부분의 CPU가 기본 크기를 int로 계산하기 때문에 short를 사용하면 int를 사용할 때보다 성능 저하가 될 수 있다.

따라서 만약 메모리의 크기가 작아서 용량을 최소한으로 사용해야 하는 경우가 아니라면 

정수의 크기가 int의 크기보다 작더라도 short 보다는 int를 사용하는 것이 좋습니다.



3-3. int


int(integer) 크기의 표준은 아래와 같다.


1. 최소 16비트

2. short 크기 이상


CPU는 메모리로부터 읽어오는 데이터나 ALU(Arithmetic Logic Unit)의 연산 결과 값을 

레지스터라는 곳에 임시로 저장하며 연산을 진행합니다. 

이때의 데이터 처리 단위를 워드(Word)라고 하는데 int가 이 워드의 크기와 동일합니다.

# 처리 단위가 같다는 것은 저장이 간편하다는 뜻인 듯..? 따로 그 자료형에 맞게 메모리 공간을 나눌 필요가 없으니까.


옛날에는 16비트 컴퓨터가 굉장히 많았기 때문에 int의 표준이 16비트 이상인 것이지만 현재는 일반적으로 32비트로 사용된다.

그런데 현재는 32비트가 아니라 64비트 컴퓨터가 일반적인데 그럼에도 int가 32비트로 사용되는 이유는 뭘까?

이미 이전까지 int가 32비트로 사용되었던 프로그램들이 너무나도 많고 

int를 64비트로 변경하면 다른 자료형들의 표준까지 어긋나 버리기 때문에 

혼동을 피하기 위하여 int를 32비트로 유지한다고 볼 수 있다.



3-4. long


long 크기의 표준은 아래와 같습니다.


1. 최소 32비트

2. int 크기 이상


다른 언어에서는 보통 long을 64비트로 사용하지만 특이하게도 C언어에서는 long도 int와 마찬가지로 32비트를 사용한다.

C는 굉장히 오래전에 만들어진 언어이기 때문에 과거에는 int가 16비트 long이 32비트로 쓰였겠지만 

현재는 int가 32비트로 쓰이면서 long과 같아진 것이라고 볼 수 있다.

# 그래서 C99에서는 64비트 정수형인 long long이 표준으로 등장합니다.



3-5. stdint.h(C99)


위와 같이 C언어에서 정수형은 각각의 타입마다 통상적으로 사용되는 범위는 있지만 표준 상 정해져 있는 것은 아니다. 

구현체에 따라 달라질 수 있고 C 프로그래머라면 얼마든지 이러한 상황이 생길 수 있다. 

따라서 char, int와 같은 가변적인 자료형을 쓰는 것이 아니라 고정적인 자료형을 사용한다면 

어떠한 구현체에서도 안전하게 작동하는 프로그램이 될 것이다.

 
따라서 그저 예제 수준의 프로그램을 작성한다면 char와 int를 사용해도 문제가 없겠지만 

이식성(Portability)이 중요한 프로그램이나 자료형의 크기가 매우 중요한 시스템, 네트워크 등의 프로그램에서는 

C99부터 지원되는 stdint.h의 int8_t, int16_t 등을 사용하여 비트 개념으로 고정되어 있는 자료형을 쓰는 것이 안전하다.




4. 실수형


위에서 살펴본 정수형은 범위가 주어지면 유한적인 자료형이다. 

예를 들어 char를 8비트로 사용한다면 표현할 수 있는 범위는 256가지로 정해진 것이죠. 

하지만 실수형은 범위가 주어진다 하더라도 무한한 자료형이다. 

0부터 1까지도 실수로 표현한다면 무한대로 표현할 수 있기 때문이다. 그런데 컴퓨터의 공간은 무한하지가 않다.

따라서 실수의 표현을 최대한으로 크게 하기 위하여 소수점은 고정시키지 않고 

유동적으로(둥둥 떠다닐 수 있게) 사용한다는 개념인 부동소수점(Floating Point)이 등장하였다. 

부동 소수점은 IEEE의 표준을 따라 IEEE 754가 일반적으로 사용된다. 

또한 unsigned형이 없기 때문에 signed나 unsigned를 표기한다면 컴파일 에러가 발생합니다.



4-1. float


하지만 C언어는 IEEE754가 적용되기 이전에 만들어진 언어이다. 따라서 표준상으로는 아래와 같다.

char 크기 이상
 

일반적인 float의 사용은 아래와 같다.


1. 크기: 32비트

2. 범위: IEEE754 Single



4-2. double


C에서 기본적으로 사용되는 실수형은 double이며 표준은 아래와 같다.

float 이상
 

일반적인 double의 사용은 아래와 같다.


1. 크기: 64비트

2. 범위: IEEE754 Double



4-3 long double


표준은 아래와 같다.

double 이상
 

long double은 다른 언어에서는 잘 없고 C에서도 잘 쓰이지는 않는다. 

구현체마다 달라지기 때문에 double과 동일할 수도 있고 그 이상일 수도 있습니다.



5. 논리형(C99)


C언어에도 논리형인 bool이 있다. 다만 C99부터 지원되기 때문에 C89 표준을 기준으로 보는 사람들은 

C에는 bool이 없다고 표현하죠. 따라서 어떠한 표준을 기준으로 C 프로그래밍을 하냐에 따라서 

매크로로 TRUE와 FALSE를 선언하여 사용할 수도 있고 아예 bool 타입을 가져다 쓸 수도 있습니다.



결론)

예를 들어 컴파일러에 따라 비트당 바이트 단위를 다르게 인식하기도 한다는 것이다.

8비트가 무조건 1바이트가 아니라는 뜻이다.


왜 굳이 8비트인가? 7bit, 16bit, 32bit 이면 안 되었던 건가? 라고 한다면

컴퓨터 아키텍쳐가 영문권인 곳에서 발전했기 때문이라고 한다.

0101010 로 이루어진 전자신호를 사람이 인식할 수 있는 문자로 저장을 했어야 했는데

이런 문자를 표현하는 코드들의 숫자가 7bit ~8bit 으로 충분했기 때문이라고 한다.

(ASCII : 제어문자(32개), 출력가능문자(영소문자, 영대문자, 숫자, 기타 기호, parity bit 등등)

현재도 ASCII extended 부호를 사용하고 있다. (7bit + 1bit)

이래서 1Byte에 문자 1개를 저장할 수 있는 8bit 구조가 된 것이라고 한다.


이렇게 가변적인 자료형은 여기서는 8비트 1바이트 기준이지만 다른 곳에는

16비트를 1바이트로 하는 경우가 있을 수 있기 때문에 이식성이나 자료형이 중요시 되는 프로그램은

고정된 기준을 삼는 자로형을 사용하면 좋다는 것이다.

그게 c99부터 있다는 듯 하다.
